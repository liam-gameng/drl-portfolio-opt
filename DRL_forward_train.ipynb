{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL Walk-Forward Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "from collections import deque\n",
    "import yfinance as yf\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA or CPU:  cuda\n",
      "Device:  NVIDIA GeForce RTX 4080\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA or CPU: \", device)\n",
    "print(\"Device: \",torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "WINDOW_SIZE = 120  # Rolling window size for state representation\n",
    "STD_WINDOW_SIZE = 20\n",
    "NUM_ASSETS = 14   # Example number of assets\n",
    "ENV_STEP_SIZE = 1\n",
    "LEARNING_RATE = 0.01\n",
    "WEIGHT_DECAY = 1e-4\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "BATCH_SIZE = 50\n",
    "EPISODES = 100\n",
    "THETA = 0.5\n",
    "SIGMA = 0.02\n",
    "LAMBDA = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  14 of 14 completed\n",
      "[**************        29%                       ]  4 of 14 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Period: 2011-01-01 to 2015-12-31 ---\n",
      "Data Start Date: 2010-01-05 00:00:00+00:00\n",
      "Data End Date: 2015-12-30 00:00:00+00:00\n",
      "Total Days in Dataset: 1508\n",
      "Lookback Start Index: 110\n",
      "Analysis Index: 250\n",
      "Filtered Data Start Date (includes look-back): 2010-06-14 00:00:00\n",
      "Filtered Data End Date: 2015-12-30 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  14 of 14 completed\n",
      "[*******               14%                       ]  2 of 14 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Period: 2012-01-01 to 2016-12-31 ---\n",
      "Data Start Date: 2011-01-04 00:00:00+00:00\n",
      "Data End Date: 2016-12-30 00:00:00+00:00\n",
      "Total Days in Dataset: 1509\n",
      "Lookback Start Index: 110\n",
      "Analysis Index: 250\n",
      "Filtered Data Start Date (includes look-back): 2011-06-13 00:00:00\n",
      "Filtered Data End Date: 2016-12-30 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  14 of 14 completed\n",
      "[                       0%                       ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Period: 2013-01-01 to 2017-12-31 ---\n",
      "Data Start Date: 2012-01-04 00:00:00+00:00\n",
      "Data End Date: 2017-12-29 00:00:00+00:00\n",
      "Total Days in Dataset: 1508\n",
      "Lookback Start Index: 108\n",
      "Analysis Index: 248\n",
      "Filtered Data Start Date (includes look-back): 2012-06-08 00:00:00\n",
      "Filtered Data End Date: 2017-12-29 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  14 of 14 completed\n",
      "[*****************     36%                       ]  5 of 14 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Period: 2014-01-01 to 2018-12-31 ---\n",
      "Data Start Date: 2013-01-03 00:00:00+00:00\n",
      "Data End Date: 2018-12-28 00:00:00+00:00\n",
      "Total Days in Dataset: 1508\n",
      "Lookback Start Index: 110\n",
      "Analysis Index: 250\n",
      "Filtered Data Start Date (includes look-back): 2013-06-12 00:00:00\n",
      "Filtered Data End Date: 2018-12-28 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  14 of 14 completed\n",
      "[*****************     36%                       ]  5 of 14 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Period: 2015-01-01 to 2019-12-31 ---\n",
      "Data Start Date: 2014-01-03 00:00:00+00:00\n",
      "Data End Date: 2019-12-30 00:00:00+00:00\n",
      "Total Days in Dataset: 1508\n",
      "Lookback Start Index: 110\n",
      "Analysis Index: 250\n",
      "Filtered Data Start Date (includes look-back): 2014-06-12 00:00:00\n",
      "Filtered Data End Date: 2019-12-30 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  14 of 14 completed\n",
      "[*******               14%                       ]  2 of 14 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Period: 2016-01-01 to 2020-12-31 ---\n",
      "Data Start Date: 2015-01-05 00:00:00+00:00\n",
      "Data End Date: 2020-12-30 00:00:00+00:00\n",
      "Total Days in Dataset: 1509\n",
      "Lookback Start Index: 110\n",
      "Analysis Index: 250\n",
      "Filtered Data Start Date (includes look-back): 2015-06-12 00:00:00\n",
      "Filtered Data End Date: 2020-12-30 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  14 of 14 completed\n",
      "[*******               14%                       ]  2 of 14 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Period: 2017-01-01 to 2021-12-31 ---\n",
      "Data Start Date: 2016-01-05 00:00:00+00:00\n",
      "Data End Date: 2021-12-30 00:00:00+00:00\n",
      "Total Days in Dataset: 1509\n",
      "Lookback Start Index: 110\n",
      "Analysis Index: 250\n",
      "Filtered Data Start Date (includes look-back): 2016-06-13 00:00:00\n",
      "Filtered Data End Date: 2021-12-30 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  14 of 14 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Period: 2018-01-01 to 2022-12-31 ---\n",
      "Data Start Date: 2017-01-04 00:00:00+00:00\n",
      "Data End Date: 2022-12-30 00:00:00+00:00\n",
      "Total Days in Dataset: 1509\n",
      "Lookback Start Index: 109\n",
      "Analysis Index: 249\n",
      "Filtered Data Start Date (includes look-back): 2017-06-12 00:00:00\n",
      "Filtered Data End Date: 2022-12-30 00:00:00\n",
      "\n",
      "Total periods processed: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define tickers\n",
    "tickers = {\n",
    "    'XLF': 'XLF',   # Financials Select Sector SPDR Fund\n",
    "    'XLK': 'XLK',   # Technology Select Sector SPDR Fund\n",
    "    'XLE': 'XLE',   # Energy Select Sector SPDR Fund\n",
    "    'XLP': 'XLP',   # Consumer Staples Select Sector SPDR Fund\n",
    "    'XLY': 'XLY',   # Consumer Discretionary Select Sector SPDR Fund\n",
    "    'XLU': 'XLU',   # Utilities Select Sector SPDR Fund\n",
    "    'XLI': 'XLI',   # Industrials Select Sector SPDR Fund\n",
    "    'XLV': 'XLV',   # Health Care Select Sector SPDR Fund\n",
    "    'VNQ': 'VNQ',   # Vanguard Real Estate ETF\n",
    "    'IYZ': 'IYZ',   # iShares U.S. Telecommunications ETF\n",
    "    'XBI': 'XBI',   # SPDR S&P Biotech ETF\n",
    "    'XOP': 'XOP',   # SPDR S&P Oil & Gas Exploration ETF\n",
    "    'ITA': 'ITA',   # iShares U.S. Aerospace & Defense ETF\n",
    "    'KBE': 'KBE'    # SPDR S&P Bank ETF\n",
    "}\n",
    "\n",
    "# Define the range of years\n",
    "start_year = 2011\n",
    "end_year = 2019\n",
    "\n",
    "# Initialize an empty list to store filtered returns\n",
    "filtered_returns_list = []\n",
    "\n",
    "# Loop through each year in the range\n",
    "for year in range(start_year, end_year):\n",
    "    # Dynamically adjust parameters for each iteration\n",
    "    start_analysis_date = f\"{year}-01-01\"\n",
    "    extra_data_start_date = f\"{year - 1}-01-01\"\n",
    "    end_date = f\"{year + 4}-12-31\"\n",
    "\n",
    "    # Download data\n",
    "    data = yf.download(list(tickers.values()), start=extra_data_start_date, end=end_date)['Adj Close']\n",
    "\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change().dropna()\n",
    "\n",
    "    # Print data summary\n",
    "    print(f\"\\n--- Period: {start_analysis_date} to {end_date} ---\")\n",
    "    print(\"Data Start Date:\", returns.index.min())\n",
    "    print(\"Data End Date:\", returns.index.max())\n",
    "    print(\"Total Days in Dataset:\", len(returns))\n",
    "\n",
    "    # Convert start_analysis_date to pandas Timestamp\n",
    "    start_analysis_date = pd.Timestamp(start_analysis_date)\n",
    "\n",
    "    # Ensure the index is timezone-naive for consistency\n",
    "    returns.index = returns.index.tz_localize(None)\n",
    "\n",
    "    # Find the index of the closest date before or equal to the start_analysis_date\n",
    "    if start_analysis_date not in returns.index:\n",
    "        start_analysis_date = returns.index.asof(start_analysis_date)\n",
    "\n",
    "    analysis_index = returns.index.get_loc(start_analysis_date)\n",
    "\n",
    "    # Compute the index for the look-back start date\n",
    "    lookback_start_index = analysis_index - (WINDOW_SIZE + STD_WINDOW_SIZE)\n",
    "\n",
    "    # Validate the computed index\n",
    "    if lookback_start_index < 0:\n",
    "        print(\"Warning: Not enough data available for the look-back period!\")\n",
    "        continue\n",
    "\n",
    "    print(\"Lookback Start Index:\", lookback_start_index)\n",
    "    print(\"Analysis Index:\", analysis_index)\n",
    "\n",
    "    # Get the look-back start date from the index\n",
    "    lookback_start_date = returns.index[lookback_start_index]\n",
    "\n",
    "    # Filter the data to include the necessary look-back period\n",
    "    filtered_returns = returns.loc[lookback_start_date:]\n",
    "    filtered_returns_list.append(filtered_returns)\n",
    "\n",
    "    # Confirm the filtered data range\n",
    "    print(f\"Filtered Data Start Date (includes look-back): {filtered_returns.index.min()}\")\n",
    "    print(f\"Filtered Data End Date: {filtered_returns.index.max()}\")\n",
    "\n",
    "# Output the number of processed periods\n",
    "print(f\"\\nTotal periods processed: {len(filtered_returns_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(data, window_size):\n",
    "    # Ensure we have enough data for the specified window sizes\n",
    "    if len(data) < STD_WINDOW_SIZE + window_size:\n",
    "        raise ValueError(\"Data length must be at least STD_WINDOW_SIZE + window_size to calculate rolling metrics.\")\n",
    "    \n",
    "    # Convert data to a tensor and move to GPU\n",
    "    data_tensor = torch.tensor(data.values, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Initialize tensors to store results for the rolling metrics\n",
    "    num_assets = data.shape[1]\n",
    "    rolling_returns = data_tensor[STD_WINDOW_SIZE:STD_WINDOW_SIZE + window_size]  # Use the returns directly\n",
    "    rolling_volatilities = torch.zeros((window_size, num_assets), device=device)\n",
    "    rolling_drawdowns = torch.zeros((window_size, num_assets), device=device)\n",
    "\n",
    "    # Calculate cumulative returns to determine drawdowns\n",
    "    cumulative_returns = torch.cumprod(1 + rolling_returns, dim=0)\n",
    "    peak_values = torch.cummax(cumulative_returns, dim=0).values\n",
    "    rolling_drawdowns = torch.where(cumulative_returns < peak_values, cumulative_returns - peak_values, torch.tensor(0.0, device=device))\n",
    "\n",
    "    # Calculate rolling volatilities using an inner window of `std_size`\n",
    "    for i in range(window_size):\n",
    "        idx = STD_WINDOW_SIZE + i\n",
    "        volatility_window = data_tensor[idx - STD_WINDOW_SIZE:idx]  # Only previous `std_size` returns for volatility\n",
    "        rolling_volatilities[i, :] = volatility_window.std(dim=0)\n",
    "\n",
    "    # Move results back to CPU if needed\n",
    "    return (rolling_returns.cpu().numpy(), \n",
    "            rolling_volatilities.cpu().numpy(), \n",
    "            rolling_drawdowns.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_matrix(returns, volatilities, drawdowns):\n",
    "    return np.stack([returns, volatilities, drawdowns], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140, 14)\n"
     ]
    }
   ],
   "source": [
    "# Get the past `WINDOW_SIZE` days of data up to `self.current_step`\n",
    "window_data = data.iloc[0:STD_WINDOW_SIZE + WINDOW_SIZE]\n",
    "print(window_data.shape)\n",
    "returns, volatilities, drawdowns = calculate_features(window_data, WINDOW_SIZE)\n",
    "        \n",
    "# Stack features to form the state, ensuring a shape of (3, 60, 11)\n",
    "state = create_state_matrix(returns, volatilities, drawdowns)\n",
    "if state.shape != (3, WINDOW_SIZE, NUM_ASSETS):\n",
    "    raise ValueError(f\"State shape is {state.shape} but expected (3, {WINDOW_SIZE}, {NUM_ASSETS})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adjusted_rewards(data, action, annualized_return, periods_per_year=252):\n",
    "    \"\"\"\n",
    "    Calculate the Sharpe ratio for a portfolio based on the given action (portfolio weights),\n",
    "    the provided annualized return, and the return data over a specified time window.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The return data for assets over a time window.\n",
    "        action (np.array): Portfolio weights for the assets.\n",
    "        annualized_return (float): Precomputed annualized return of the portfolio.\n",
    "        periods_per_year (int): Number of periods in a year (e.g., 252 for daily trading).\n",
    "        \n",
    "    Returns:\n",
    "        adjusted_reward (float): Adjusted returns of the portfolio.\n",
    "    \"\"\"\n",
    "\n",
    "    returns = annualized_return / periods_per_year\n",
    "\n",
    "    # Convert data to a tensor\n",
    "    data_tensor = torch.tensor(data.values, dtype=torch.float32).to(device)  # Shape: (time_window, num_assets)\n",
    "\n",
    "    # Compute portfolio returns over the time window\n",
    "    action_tensor = torch.tensor(action, dtype=torch.float32).to(device)  # Shape: (num_assets,)\n",
    "    portfolio_returns = data_tensor @ action_tensor  # Shape: (time_window,)\n",
    "\n",
    "    # Calculate the standard deviation of portfolio returns\n",
    "    std_return = portfolio_returns.std().item()\n",
    "\n",
    "    # Calculate Sharpe ratio, handling cases where annualized_std is zero\n",
    "    adjusted_reward = returns - (LAMBDA / 2) * std_return**2\n",
    "\n",
    "    return adjusted_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data.dropna()  # Drop initial NaNs to ensure complete data\n",
    "        self.current_step = STD_WINDOW_SIZE + WINDOW_SIZE  # Start with enough data for the rolling window\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset to the initial starting point, which is the 2*WINDOW_SIZE day\n",
    "        self.current_step = STD_WINDOW_SIZE + WINDOW_SIZE\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Calculate the reward (e.g., based on Sharpe ratio) for the next WINDOW_SIZE days\n",
    "        portfolio_returns = self.get_portfolio_return(action)\n",
    "        vol_window = self.data.iloc[self.current_step - STD_WINDOW_SIZE + 1:self.current_step + 1]\n",
    "        reward = calculate_adjusted_rewards(vol_window, action, portfolio_returns)  # Reward based on Sharpe ratio\n",
    "        \n",
    "        # Move to the next day, and check the updated `done` condition\n",
    "        self.current_step += ENV_STEP_SIZE\n",
    "        self.done = self.current_step + ENV_STEP_SIZE >= len(self.data)  # Ensure enough data for future window\n",
    "        return self.get_state(), reward, self.done, {}\n",
    "\n",
    "    def get_state(self):\n",
    "        # Get a consistent rolling window of data ending at `self.current_step`\n",
    "        if self.current_step < STD_WINDOW_SIZE + WINDOW_SIZE:\n",
    "            raise ValueError(\"current_step must be at least STD_WINDOW_SIZE + WINDOW_SIZE to have a full double-window.\")\n",
    "\n",
    "        # Get the past `2 * WINDOW_SIZE` days of data up to `self.current_step`\n",
    "        window_data = self.data.iloc[self.current_step - STD_WINDOW_SIZE - WINDOW_SIZE:self.current_step]\n",
    "        returns, volatilities, drawdowns = calculate_features(window_data, WINDOW_SIZE)\n",
    "        \n",
    "        # Stack features to form the state, ensuring a shape of (3, WINDOW_SIZE, num_assets)\n",
    "        state = create_state_matrix(returns, volatilities, drawdowns)\n",
    "        if state.shape != (3, WINDOW_SIZE, NUM_ASSETS):\n",
    "            raise ValueError(f\"State shape is {state.shape} but expected (3, {WINDOW_SIZE}, {NUM_ASSETS})\")\n",
    "        \n",
    "        # Calculate the rolling covariance matrix\n",
    "        cov = data.iloc[self.current_step - WINDOW_SIZE:self.current_step].cov()\n",
    "\n",
    "        # Store the covariance matrix as part of the state\n",
    "        state = {\n",
    "            'features': state,  # Original features (returns, volatilities, drawdowns)\n",
    "            'covariance': cov.values  # Covariance matrix\n",
    "        }\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def get_portfolio_return(self, action, periods_per_year=252):\n",
    "        # Ensure there is a next day available\n",
    "        if self.current_step + 1 >= len(self.data):\n",
    "            raise ValueError(\"Not enough data to calculate next day's return.\")\n",
    "\n",
    "        # Calculate returns for the next day (current_step + 1)\n",
    "        next_day_returns = self.data.iloc[self.current_step + 1]\n",
    "\n",
    "        # Portfolio return as dot product of action weights and next day's returns\n",
    "        portfolio_return = np.dot(action, next_day_returns) * periods_per_year\n",
    "\n",
    "        return portfolio_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, num_assets, window_size=WINDOW_SIZE, hidden_dim=256):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        # Define convolutional layers for the primary features (returns, volatilities, drawdowns)\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1).to(device)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1).to(device)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, padding=1).to(device)\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, padding=1).to(device)  # Updated input channels to match\n",
    "        self.conv5 = nn.Conv2d(64, 64, kernel_size=3, padding=1).to(device)\n",
    "        self.conv6 = nn.Conv2d(64, 128, kernel_size=3, padding=1).to(device)  # Increased output channels for more complexity\n",
    "\n",
    "        # Initialize fully connected layers for feature branch\n",
    "        self._initialize_feature_branch(num_assets, window_size)\n",
    "\n",
    "        # Flattened covariance size (num_assets * num_assets)\n",
    "        self.covariance_size = num_assets * num_assets\n",
    "\n",
    "        # Define final fully connected layers for combined output\n",
    "        combined_size = self.feature_output_size + self.covariance_size\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(combined_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_assets),\n",
    "            nn.Softmax(dim=-1)  # Output portfolio weights\n",
    "        ).to(device)\n",
    "\n",
    "    def _initialize_feature_branch(self, num_assets, window_size):\n",
    "        # Calculate flatten size for the feature branch\n",
    "        dummy_input = torch.zeros(1, 3, window_size, num_assets).to(device)\n",
    "        x = torch.relu(self.conv1(dummy_input))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = torch.relu(self.conv6(x))\n",
    "        self.feature_output_size = x.view(1, -1).shape[1]\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Process primary features\n",
    "        features = state['features'].to(device)\n",
    "        x = torch.relu(self.conv1(features))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = torch.relu(self.conv6(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten feature branch output\n",
    "\n",
    "        # Flatten covariance matrix\n",
    "        covariance = state['covariance'].to(device).view(x.size(0), -1)  # Flatten covariance matrix\n",
    "\n",
    "        # Combine both branches\n",
    "        combined = torch.cat([x, covariance], dim=1)\n",
    "\n",
    "        # Final output layer\n",
    "        weights = self.fc(combined)\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, num_assets, window_size=WINDOW_SIZE, hidden_dim=256):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        # Convolutional layers for processing the state (features: returns, volatilities, drawdowns)\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1).to(device)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1).to(device)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, padding=1).to(device)\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, padding=1).to(device)  # Updated input channels to match conv3\n",
    "        self.conv5 = nn.Conv2d(64, 64, kernel_size=3, padding=1).to(device)\n",
    "        self.conv6 = nn.Conv2d(64, 128, kernel_size=3, padding=1).to(device)  # Increased output channels for more complexity\n",
    "\n",
    "        # Initialize fully connected layers for the feature branch\n",
    "        self._initialize_feature_branch(num_assets, window_size)\n",
    "\n",
    "        # Flattened covariance size (num_assets * num_assets)\n",
    "        self.covariance_size = num_assets * num_assets\n",
    "\n",
    "        # Fully connected layers for action processing\n",
    "        self.fc_action = nn.Sequential(\n",
    "            nn.Linear(num_assets, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        ).to(device)\n",
    "\n",
    "        # Final layers for combining all representations (features, covariance, action)\n",
    "        combined_size = self.feature_output_size + self.covariance_size + hidden_dim\n",
    "        self.fc_q = nn.Sequential(\n",
    "            nn.Linear(combined_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # Output Q-value\n",
    "        ).to(device)\n",
    "\n",
    "    def _initialize_feature_branch(self, num_assets, window_size):\n",
    "        # Calculate flatten size for the feature branch\n",
    "        dummy_input = torch.zeros(1, 3, window_size, num_assets).to(device)\n",
    "        x = torch.relu(self.conv1(dummy_input))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = torch.relu(self.conv6(x))\n",
    "        self.feature_output_size = x.view(1, -1).shape[1]\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # Process the state (features: returns, volatilities, drawdowns)\n",
    "        features = state['features'].to(device)\n",
    "        x = torch.relu(self.conv1(features))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = torch.relu(self.conv6(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten feature branch output\n",
    "        state_out = x\n",
    "\n",
    "        # Flatten covariance matrix\n",
    "        covariance = state['covariance'].to(device).view(x.size(0), -1)  # Flatten covariance matrix\n",
    "        cov_out = covariance\n",
    "\n",
    "        # Process the action\n",
    "        action_out = self.fc_action(action.to(device))\n",
    "\n",
    "        # Combine all branches\n",
    "        combined = torch.cat([state_out, cov_out, action_out], dim=1)\n",
    "\n",
    "        # Compute Q-value\n",
    "        q_value = self.fc_q(combined)\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    def __init__(self, size, mu=0.0, theta=0.15, sigma=0.2):\n",
    "        \"\"\"\n",
    "        Ornstein-Uhlenbeck noise process.\n",
    "        Args:\n",
    "            size (int): Dimension of the noise.\n",
    "            mu (float): Mean of the process.\n",
    "            theta (float): Speed of mean reversion.\n",
    "            sigma (float): Volatility parameter.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.size) * self.mu\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state to the mean.\"\"\"\n",
    "        self.state = np.ones(self.size) * self.mu\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Generate a noise sample.\"\"\"\n",
    "        dx = self.theta * (self.mu - self.state) + self.sigma * np.random.normal(size=self.size)\n",
    "        self.state += dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, num_assets):\n",
    "        # Initialize actor and critic networks, and target networks\n",
    "        self.actor = ActorNetwork(num_assets).to(device)\n",
    "        self.critic = CriticNetwork(num_assets).to(device)\n",
    "        self.target_actor = ActorNetwork(num_assets).to(device)\n",
    "        self.target_critic = CriticNetwork(num_assets).to(device)\n",
    "\n",
    "        # Synchronize target networks with the main networks\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.discount_factor = DISCOUNT_FACTOR\n",
    "        self.tau = 0.005  # Soft update rate for target networks\n",
    "\n",
    "        # OU Noise for exploration\n",
    "        self.ou_noise = OUNoise(size=num_assets, theta=THETA, sigma=SIGMA)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"\n",
    "        Get an action from the actor network, optionally adding noise.\n",
    "        Args:\n",
    "            state (np.array): Current state.\n",
    "            add_noise (bool): Whether to add exploration noise.\n",
    "        Returns:\n",
    "            np.array: Action vector.\n",
    "        \"\"\"\n",
    "        # Convert state components to tensors and move to GPU if available\n",
    "        features_tensor = torch.tensor(state['features'], dtype=torch.float32).unsqueeze(0).to(device)  # Shape: (1, 3, WINDOW_SIZE, NUM_ASSETS)\n",
    "        covariance_tensor = torch.tensor(state['covariance'], dtype=torch.float32).unsqueeze(0).to(device)  # Shape: (1, 1, NUM_ASSETS, NUM_ASSETS)\n",
    "\n",
    "        # Combine tensors into a dictionary to match the actor's expected input\n",
    "        state_tensor = {\n",
    "            'features': features_tensor,\n",
    "            'covariance': covariance_tensor\n",
    "        }\n",
    "\n",
    "        # Pass the structured state to the actor and get the action\n",
    "        action = self.actor(state_tensor).detach().cpu().numpy()[0]  # Move action back to CPU\n",
    "\n",
    "        # Optionally add Ornstein-Uhlenbeck noise for exploration\n",
    "        if add_noise:\n",
    "            noise = self.ou_noise.sample()\n",
    "            action += noise\n",
    "\n",
    "        action = np.clip(action, 0, 1)  # Ensure valid portfolio weights \n",
    "        return action / action.sum()  # Ensure weights sum to 1\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Reset the Ornstein-Uhlenbeck noise process.\"\"\"\n",
    "        self.ou_noise.reset()\n",
    "\n",
    "    def update(self):\n",
    "        # Check if there are enough samples in memory\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Sample a mini-batch of experiences from the replay buffer\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*mini_batch)\n",
    "\n",
    "        # Convert rewards and dones to tensors\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32).view(-1, 1).to(device)\n",
    "        dones = torch.tensor(np.array(dones), dtype=torch.float32).view(-1, 1).to(device)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.float32).to(device)\n",
    "\n",
    "        # Process states and next_states into features and covariance components\n",
    "        states_features = torch.tensor(np.array([s['features'] for s in states]), dtype=torch.float32).to(device)\n",
    "        states_covariance = torch.tensor(np.array([s['covariance'] for s in states]), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "        next_states_features = torch.tensor(np.array([s['features'] for s in next_states]), dtype=torch.float32).to(device)\n",
    "        next_states_covariance = torch.tensor(np.array([s['covariance'] for s in next_states]), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "        # Create structured state dictionaries for compatibility with the networks\n",
    "        states_dict = {'features': states_features, 'covariance': states_covariance}\n",
    "        next_states_dict = {'features': next_states_features, 'covariance': next_states_covariance}\n",
    "\n",
    "        # Critic update\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.target_actor(next_states_dict)\n",
    "            target_q_values = self.target_critic(next_states_dict, next_actions)\n",
    "            target_values = rewards + self.discount_factor * target_q_values * (1 - dones)\n",
    "\n",
    "        current_q_values = self.critic(states_dict, actions)\n",
    "        critic_loss = nn.MSELoss()(current_q_values, target_values)\n",
    "\n",
    "        # Backpropagate the loss for the critic network\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "\n",
    "        # Actor update\n",
    "        predicted_actions = self.actor(states_dict)\n",
    "        actor_loss = -self.critic(states_dict, predicted_actions).mean()  # Maximize Q-value\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "\n",
    "        # Soft update for target networks\n",
    "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training for Period 1 ---\n",
      "Loaded actor model from saved_models/last_actor_7-1.pth and critic model from saved_models/last_critic_7-1.pth\n",
      "Iteration 1, Episode 1/100, Average Reward: 0.11322103544715809\n",
      "Iteration 1, Episode 2/100, Average Reward: 0.09901942546390542\n",
      "Iteration 1, Episode 3/100, Average Reward: 0.10866979610205244\n",
      "Iteration 1, Episode 4/100, Average Reward: 0.051840591096521345\n",
      "Iteration 1, Episode 5/100, Average Reward: 0.09779456549384487\n",
      "Iteration 1, Episode 6/100, Average Reward: 0.10915470368685194\n",
      "Iteration 1, Episode 7/100, Average Reward: 0.10110961674711855\n",
      "Iteration 1, Episode 8/100, Average Reward: 0.11251596563351801\n",
      "Iteration 1, Episode 9/100, Average Reward: 0.09789016253836323\n",
      "Iteration 1, Episode 10/100, Average Reward: 0.13210296236783609\n",
      "Iteration 1, Episode 11/100, Average Reward: 0.043017163346862834\n",
      "Iteration 1, Episode 12/100, Average Reward: 0.09327386749802435\n",
      "Iteration 1, Episode 13/100, Average Reward: 0.1018528778744472\n",
      "Iteration 1, Episode 14/100, Average Reward: 0.09940977014046869\n",
      "Iteration 1, Episode 15/100, Average Reward: 0.10013185824688975\n",
      "Iteration 1, Episode 16/100, Average Reward: 0.09638230769859789\n",
      "Iteration 1, Episode 17/100, Average Reward: 0.09933804967526524\n",
      "Iteration 1, Episode 18/100, Average Reward: 0.09393033067085572\n",
      "Iteration 1, Episode 19/100, Average Reward: 0.09469436290337822\n",
      "Iteration 1, Episode 20/100, Average Reward: 0.11340583307238813\n",
      "Iteration 1, Episode 21/100, Average Reward: 0.060669769488570946\n",
      "Iteration 1, Episode 22/100, Average Reward: 0.13418250646731472\n",
      "Iteration 1, Episode 23/100, Average Reward: 0.1021709058538307\n",
      "Iteration 1, Episode 24/100, Average Reward: 0.0959457970357156\n",
      "Iteration 1, Episode 25/100, Average Reward: 0.10413272941372449\n",
      "Iteration 1, Episode 26/100, Average Reward: 0.06613206919348599\n",
      "Iteration 1, Episode 27/100, Average Reward: 0.10844203891628904\n",
      "Iteration 1, Episode 28/100, Average Reward: -0.08553793315223548\n",
      "Iteration 1, Episode 29/100, Average Reward: 0.09691322291934104\n",
      "Iteration 1, Episode 30/100, Average Reward: 0.141272304755458\n",
      "Iteration 1, Episode 31/100, Average Reward: 0.05589304308298073\n",
      "Iteration 1, Episode 32/100, Average Reward: 0.08626608193621228\n",
      "Iteration 1, Episode 33/100, Average Reward: 0.08188923953842932\n",
      "Iteration 1, Episode 34/100, Average Reward: 0.048767119370152\n",
      "Iteration 1, Episode 35/100, Average Reward: 0.07581444386087942\n",
      "Iteration 1, Episode 36/100, Average Reward: 0.07648574174889501\n",
      "Iteration 1, Episode 37/100, Average Reward: 0.09127644409846408\n",
      "Iteration 1, Episode 38/100, Average Reward: 0.04911892838006498\n",
      "Iteration 1, Episode 39/100, Average Reward: 0.04214491361931486\n",
      "Iteration 1, Episode 40/100, Average Reward: 0.05121943875225108\n",
      "Iteration 1, Episode 41/100, Average Reward: 0.0791020587769469\n",
      "Iteration 1, Episode 42/100, Average Reward: 0.07491135922258636\n",
      "Iteration 1, Episode 43/100, Average Reward: 0.10061369591757328\n",
      "Iteration 1, Episode 44/100, Average Reward: 0.09470428556191887\n",
      "Iteration 1, Episode 45/100, Average Reward: 0.05966856373402026\n",
      "Iteration 1, Episode 46/100, Average Reward: 0.0959295975916743\n",
      "Iteration 1, Episode 47/100, Average Reward: 0.10556193667592335\n",
      "Iteration 1, Episode 48/100, Average Reward: 0.0834610330837265\n",
      "Iteration 1, Episode 49/100, Average Reward: 0.06732692324406192\n",
      "Iteration 1, Episode 50/100, Average Reward: 0.08352350608536682\n",
      "Iteration 1, Episode 51/100, Average Reward: 0.09455822432237893\n",
      "Iteration 1, Episode 52/100, Average Reward: 0.10394668697314115\n",
      "Iteration 1, Episode 53/100, Average Reward: 0.03233772574686189\n",
      "Iteration 1, Episode 54/100, Average Reward: 0.06275940081302901\n",
      "Iteration 1, Episode 55/100, Average Reward: 0.054279679628624386\n",
      "Iteration 1, Episode 56/100, Average Reward: 0.06057409788443661\n",
      "Iteration 1, Episode 57/100, Average Reward: 0.10420065825970405\n",
      "Iteration 1, Episode 58/100, Average Reward: 0.10375549996484913\n",
      "Iteration 1, Episode 59/100, Average Reward: 0.10258308785187475\n",
      "Iteration 1, Episode 60/100, Average Reward: 0.10248152167820122\n",
      "Iteration 1, Episode 61/100, Average Reward: 0.0986187985251231\n",
      "Iteration 1, Episode 62/100, Average Reward: 0.07939358105955394\n",
      "Iteration 1, Episode 63/100, Average Reward: 0.09407449020921024\n",
      "Iteration 1, Episode 64/100, Average Reward: 0.10046814719794299\n",
      "Iteration 1, Episode 65/100, Average Reward: 0.10286211260391274\n",
      "Iteration 1, Episode 66/100, Average Reward: 0.09527981618313072\n",
      "Iteration 1, Episode 67/100, Average Reward: 0.09717019777879897\n",
      "Iteration 1, Episode 68/100, Average Reward: 0.10429268634253654\n",
      "Iteration 1, Episode 69/100, Average Reward: 0.11664140526011992\n",
      "Iteration 1, Episode 70/100, Average Reward: 0.10028014582860613\n",
      "Iteration 1, Episode 71/100, Average Reward: 0.11851390892599488\n",
      "Iteration 1, Episode 72/100, Average Reward: -0.09142815428139156\n",
      "Iteration 1, Episode 73/100, Average Reward: 0.04999722224454524\n",
      "Iteration 1, Episode 74/100, Average Reward: -0.06545691070718263\n",
      "Iteration 1, Episode 75/100, Average Reward: 0.1455982918854604\n",
      "Iteration 1, Episode 76/100, Average Reward: 0.12750601351808216\n",
      "Iteration 1, Episode 77/100, Average Reward: 0.17063359383307863\n",
      "Iteration 1, Episode 78/100, Average Reward: 0.05233988922738123\n",
      "Iteration 1, Episode 79/100, Average Reward: 0.10124471382001785\n",
      "Iteration 1, Episode 80/100, Average Reward: 0.07693995996402421\n",
      "Iteration 1, Episode 81/100, Average Reward: 0.1381439846152581\n",
      "Iteration 1, Episode 82/100, Average Reward: 0.10692479104985746\n",
      "Iteration 1, Episode 83/100, Average Reward: 0.09847914298010406\n",
      "Iteration 1, Episode 84/100, Average Reward: 0.09640952913865287\n",
      "Iteration 1, Episode 85/100, Average Reward: 0.09415529811871524\n",
      "Iteration 1, Episode 86/100, Average Reward: -0.08668716025691629\n",
      "Iteration 1, Episode 87/100, Average Reward: 0.03470607266491597\n",
      "Iteration 1, Episode 88/100, Average Reward: 0.0921228220259861\n",
      "Iteration 1, Episode 89/100, Average Reward: 0.10808328887845912\n",
      "Iteration 1, Episode 90/100, Average Reward: 0.10127179143149856\n",
      "Iteration 1, Episode 91/100, Average Reward: 0.09846645514961402\n",
      "Iteration 1, Episode 92/100, Average Reward: 0.09685650808518297\n",
      "Iteration 1, Episode 93/100, Average Reward: 0.09796269662324501\n",
      "Iteration 1, Episode 94/100, Average Reward: 0.1038615647658762\n",
      "Iteration 1, Episode 95/100, Average Reward: 0.10178979132622848\n",
      "Iteration 1, Episode 96/100, Average Reward: 0.11095410574066136\n",
      "Iteration 1, Episode 97/100, Average Reward: 0.09954402096298363\n",
      "Iteration 1, Episode 98/100, Average Reward: 0.09495384846806428\n",
      "Iteration 1, Episode 99/100, Average Reward: 0.21552852651125246\n",
      "Iteration 1, Episode 100/100, Average Reward: 0.0960404659060429\n",
      "\n",
      "--- Training for Period 2 ---\n",
      "Loaded actor model from saved_models/last_actor_7-2.pth and critic model from saved_models/last_critic_7-2.pth\n",
      "Iteration 2, Episode 1/100, Average Reward: 0.1401207575654566\n",
      "Iteration 2, Episode 2/100, Average Reward: 0.12819288092047004\n",
      "Iteration 2, Episode 3/100, Average Reward: 0.11851197051324948\n",
      "Iteration 2, Episode 4/100, Average Reward: 0.1342742539205816\n",
      "Iteration 2, Episode 5/100, Average Reward: 0.12658548028247943\n",
      "Iteration 2, Episode 6/100, Average Reward: 0.12910430532112174\n",
      "Iteration 2, Episode 7/100, Average Reward: 0.1401983121004115\n",
      "Iteration 2, Episode 8/100, Average Reward: 0.11981065741209518\n",
      "Iteration 2, Episode 9/100, Average Reward: 0.12584813774277373\n",
      "Iteration 2, Episode 10/100, Average Reward: 0.11858441766598023\n",
      "Iteration 2, Episode 11/100, Average Reward: 0.130272633729894\n",
      "Iteration 2, Episode 12/100, Average Reward: 0.13181251498491278\n",
      "Iteration 2, Episode 13/100, Average Reward: 0.14605787868606426\n",
      "Iteration 2, Episode 14/100, Average Reward: 0.13483353686675623\n",
      "Iteration 2, Episode 15/100, Average Reward: 0.16499963256903677\n",
      "Iteration 2, Episode 16/100, Average Reward: 0.12079081181845082\n",
      "Iteration 2, Episode 17/100, Average Reward: 0.15874097810215615\n",
      "Iteration 2, Episode 18/100, Average Reward: 0.02926472556851833\n",
      "Iteration 2, Episode 19/100, Average Reward: 0.10790342828395942\n",
      "Iteration 2, Episode 20/100, Average Reward: 0.16251369454873085\n",
      "Iteration 2, Episode 21/100, Average Reward: 0.1581611637801038\n",
      "Iteration 2, Episode 22/100, Average Reward: 0.13792931136271525\n",
      "Iteration 2, Episode 23/100, Average Reward: 0.13935228447153705\n",
      "Iteration 2, Episode 24/100, Average Reward: 0.1029151333522303\n",
      "Iteration 2, Episode 25/100, Average Reward: 0.1377678789273743\n",
      "Iteration 2, Episode 26/100, Average Reward: 0.1321992220362676\n",
      "Iteration 2, Episode 27/100, Average Reward: 0.13215153683252262\n",
      "Iteration 2, Episode 28/100, Average Reward: 0.15401826209035718\n",
      "Iteration 2, Episode 29/100, Average Reward: 0.11635103837087164\n",
      "Iteration 2, Episode 30/100, Average Reward: 0.14931755181881387\n",
      "Iteration 2, Episode 31/100, Average Reward: 0.12069352794240173\n",
      "Iteration 2, Episode 32/100, Average Reward: 0.1303026510419662\n",
      "Iteration 2, Episode 33/100, Average Reward: 0.12863185083782255\n",
      "Iteration 2, Episode 34/100, Average Reward: 0.13128644738252196\n",
      "Iteration 2, Episode 35/100, Average Reward: 0.12654392201517176\n",
      "Iteration 2, Episode 36/100, Average Reward: 0.12692520301264879\n",
      "Iteration 2, Episode 37/100, Average Reward: 0.13194298466627152\n",
      "Iteration 2, Episode 38/100, Average Reward: 0.11643617026938713\n",
      "Iteration 2, Episode 39/100, Average Reward: 0.1251136311499632\n",
      "Iteration 2, Episode 40/100, Average Reward: 0.13281788106632897\n",
      "Iteration 2, Episode 41/100, Average Reward: 0.11928503744779151\n",
      "Iteration 2, Episode 42/100, Average Reward: 0.123757070829123\n",
      "Iteration 2, Episode 43/100, Average Reward: 0.12341486683091886\n",
      "Iteration 2, Episode 44/100, Average Reward: 0.1573244954309801\n",
      "Iteration 2, Episode 45/100, Average Reward: 0.15299552595819496\n",
      "Iteration 2, Episode 46/100, Average Reward: 0.046631125831464226\n",
      "Iteration 2, Episode 47/100, Average Reward: 0.18095720890701333\n",
      "Iteration 2, Episode 48/100, Average Reward: 0.14123438600476518\n",
      "Iteration 2, Episode 49/100, Average Reward: 0.08360598554272627\n",
      "Iteration 2, Episode 50/100, Average Reward: 0.15485240381778925\n",
      "Iteration 2, Episode 51/100, Average Reward: 0.12619346313318963\n",
      "Iteration 2, Episode 52/100, Average Reward: 0.12516704760828523\n",
      "Iteration 2, Episode 53/100, Average Reward: 0.12816720986335692\n",
      "Iteration 2, Episode 54/100, Average Reward: 0.13191165406615588\n",
      "Iteration 2, Episode 55/100, Average Reward: 0.13153223510675316\n",
      "Iteration 2, Episode 56/100, Average Reward: 0.11798019168652976\n",
      "Iteration 2, Episode 57/100, Average Reward: 0.17513352713353564\n",
      "Iteration 2, Episode 58/100, Average Reward: 0.15388963990209062\n",
      "Iteration 2, Episode 59/100, Average Reward: 0.16747562839096364\n",
      "Iteration 2, Episode 60/100, Average Reward: 0.11993942873197411\n",
      "Iteration 2, Episode 61/100, Average Reward: 0.10466389437150453\n",
      "Iteration 2, Episode 62/100, Average Reward: 0.1724260016315568\n",
      "Iteration 2, Episode 63/100, Average Reward: 0.15861657406157623\n",
      "Iteration 2, Episode 64/100, Average Reward: 0.15584333406570947\n",
      "Iteration 2, Episode 65/100, Average Reward: 0.14670140638594031\n",
      "Iteration 2, Episode 66/100, Average Reward: 0.19827795828110228\n",
      "Iteration 2, Episode 67/100, Average Reward: 0.1602619319465536\n",
      "Iteration 2, Episode 68/100, Average Reward: 0.14856357009641147\n",
      "Iteration 2, Episode 69/100, Average Reward: 0.13070271336216488\n",
      "Iteration 2, Episode 70/100, Average Reward: 0.17960745228704056\n",
      "Iteration 2, Episode 71/100, Average Reward: 0.11745816902659142\n",
      "Iteration 2, Episode 72/100, Average Reward: 0.08864345148404591\n",
      "Iteration 2, Episode 73/100, Average Reward: 0.11481308151476574\n",
      "Iteration 2, Episode 74/100, Average Reward: 0.11598762771214295\n",
      "Iteration 2, Episode 75/100, Average Reward: 0.1667327397085042\n",
      "Iteration 2, Episode 76/100, Average Reward: 0.1279174236227839\n",
      "Iteration 2, Episode 77/100, Average Reward: 0.1310840600427446\n",
      "Iteration 2, Episode 78/100, Average Reward: 0.12968013824708463\n",
      "Iteration 2, Episode 79/100, Average Reward: 0.054921288321194854\n",
      "Iteration 2, Episode 80/100, Average Reward: 0.14896097314372939\n",
      "Iteration 2, Episode 81/100, Average Reward: 0.10286475173719235\n",
      "Iteration 2, Episode 82/100, Average Reward: 0.13640325532511746\n",
      "Iteration 2, Episode 83/100, Average Reward: 0.11844420093641524\n",
      "Iteration 2, Episode 84/100, Average Reward: 0.133134727912121\n",
      "Iteration 2, Episode 85/100, Average Reward: 0.13217627087999145\n",
      "Iteration 2, Episode 86/100, Average Reward: 0.13011449900668245\n",
      "Iteration 2, Episode 87/100, Average Reward: 0.12602123746191718\n",
      "Iteration 2, Episode 88/100, Average Reward: 0.12091700780031515\n",
      "Iteration 2, Episode 89/100, Average Reward: 0.047611044237764744\n",
      "Iteration 2, Episode 90/100, Average Reward: 0.11242881259007707\n",
      "Iteration 2, Episode 91/100, Average Reward: 0.12402788459888162\n",
      "Iteration 2, Episode 92/100, Average Reward: 0.10262987744757138\n",
      "Iteration 2, Episode 93/100, Average Reward: 0.07161961795450891\n",
      "Iteration 2, Episode 94/100, Average Reward: 0.13542117937427312\n",
      "Iteration 2, Episode 95/100, Average Reward: 0.12057217617734241\n",
      "Iteration 2, Episode 96/100, Average Reward: 0.12849644070253466\n",
      "Iteration 2, Episode 97/100, Average Reward: 0.13639685290557846\n",
      "Iteration 2, Episode 98/100, Average Reward: 0.12229218117280306\n",
      "Iteration 2, Episode 99/100, Average Reward: 0.12044069666900556\n",
      "Iteration 2, Episode 100/100, Average Reward: 0.11483606808586914\n",
      "\n",
      "--- Training for Period 3 ---\n",
      "Loaded actor model from saved_models/last_actor_7-3.pth and critic model from saved_models/last_critic_7-3.pth\n",
      "Iteration 3, Episode 1/100, Average Reward: 0.08367453199893127\n",
      "Iteration 3, Episode 2/100, Average Reward: 0.11937856921903085\n",
      "Iteration 3, Episode 3/100, Average Reward: 0.13814682664172961\n",
      "Iteration 3, Episode 4/100, Average Reward: 0.1349423409028685\n",
      "Iteration 3, Episode 5/100, Average Reward: 0.12685982051153313\n",
      "Iteration 3, Episode 6/100, Average Reward: 0.12432984064726442\n",
      "Iteration 3, Episode 7/100, Average Reward: 0.14056927104698633\n",
      "Iteration 3, Episode 8/100, Average Reward: 0.12495011003804568\n",
      "Iteration 3, Episode 9/100, Average Reward: 0.1512021552763117\n",
      "Iteration 3, Episode 10/100, Average Reward: 0.12193295196170104\n",
      "Iteration 3, Episode 11/100, Average Reward: 0.12519336201563097\n",
      "Iteration 3, Episode 12/100, Average Reward: 0.1254252939376022\n",
      "Iteration 3, Episode 13/100, Average Reward: 0.10735705827283028\n",
      "Iteration 3, Episode 14/100, Average Reward: 0.12909190218045097\n",
      "Iteration 3, Episode 15/100, Average Reward: 0.12391525052917328\n",
      "Iteration 3, Episode 16/100, Average Reward: 0.1292730368777396\n",
      "Iteration 3, Episode 17/100, Average Reward: 0.1372259090031074\n",
      "Iteration 3, Episode 18/100, Average Reward: 0.13170151413272618\n",
      "Iteration 3, Episode 19/100, Average Reward: 0.12613103830833805\n",
      "Iteration 3, Episode 20/100, Average Reward: 0.13448340370835724\n",
      "Iteration 3, Episode 21/100, Average Reward: 0.12810292635339257\n",
      "Iteration 3, Episode 22/100, Average Reward: 0.1350218528575101\n",
      "Iteration 3, Episode 23/100, Average Reward: 0.14213761489282048\n",
      "Iteration 3, Episode 24/100, Average Reward: 0.1315223043811483\n",
      "Iteration 3, Episode 25/100, Average Reward: 0.12824248763323176\n",
      "Iteration 3, Episode 26/100, Average Reward: 0.11818913751918049\n",
      "Iteration 3, Episode 27/100, Average Reward: 0.13467385035648216\n",
      "Iteration 3, Episode 28/100, Average Reward: 0.12550055803253424\n",
      "Iteration 3, Episode 29/100, Average Reward: 0.12758927226645045\n",
      "Iteration 3, Episode 30/100, Average Reward: 0.12314577535522492\n",
      "Iteration 3, Episode 31/100, Average Reward: 0.09587896000817188\n",
      "Iteration 3, Episode 32/100, Average Reward: 0.12736057705203277\n",
      "Iteration 3, Episode 33/100, Average Reward: 0.12723763970170204\n",
      "Iteration 3, Episode 34/100, Average Reward: 0.12584386569607173\n",
      "Iteration 3, Episode 35/100, Average Reward: 0.15651221277129804\n",
      "Iteration 3, Episode 36/100, Average Reward: 0.14834941070253052\n",
      "Iteration 3, Episode 37/100, Average Reward: 0.14231406326627763\n",
      "Iteration 3, Episode 38/100, Average Reward: 0.1340493020518735\n",
      "Iteration 3, Episode 39/100, Average Reward: 0.0971376322664576\n",
      "Iteration 3, Episode 40/100, Average Reward: 0.12172335909390553\n",
      "Iteration 3, Episode 41/100, Average Reward: 0.1333833504405357\n",
      "Iteration 3, Episode 42/100, Average Reward: 0.166597619654565\n",
      "Iteration 3, Episode 43/100, Average Reward: 0.11611132674747288\n",
      "Iteration 3, Episode 44/100, Average Reward: 0.21485578053382334\n",
      "Iteration 3, Episode 45/100, Average Reward: 0.1769922572159995\n",
      "Iteration 3, Episode 46/100, Average Reward: 0.1669980276148207\n",
      "Iteration 3, Episode 47/100, Average Reward: 0.12657304246995502\n",
      "Iteration 3, Episode 48/100, Average Reward: -0.06017803880540352\n",
      "Iteration 3, Episode 49/100, Average Reward: 0.16642086169235246\n",
      "Iteration 3, Episode 50/100, Average Reward: 0.12688421792634308\n",
      "Iteration 3, Episode 51/100, Average Reward: 0.12507279765068965\n",
      "Iteration 3, Episode 52/100, Average Reward: 0.12998577743048984\n",
      "Iteration 3, Episode 53/100, Average Reward: 0.11474294606950983\n",
      "Iteration 3, Episode 54/100, Average Reward: 0.12184925906826381\n",
      "Iteration 3, Episode 55/100, Average Reward: 0.13563843630407177\n",
      "Iteration 3, Episode 56/100, Average Reward: 0.12073545977071967\n",
      "Iteration 3, Episode 57/100, Average Reward: 0.12399656696175623\n",
      "Iteration 3, Episode 58/100, Average Reward: 0.13667168344871927\n",
      "Iteration 3, Episode 59/100, Average Reward: 0.12514918998976493\n",
      "Iteration 3, Episode 60/100, Average Reward: 0.13575744953396585\n",
      "Iteration 3, Episode 61/100, Average Reward: 0.13506597538824955\n",
      "Iteration 3, Episode 62/100, Average Reward: 0.12100153317596138\n",
      "Iteration 3, Episode 63/100, Average Reward: 0.13020069115694363\n",
      "Iteration 3, Episode 64/100, Average Reward: 0.12256542326707265\n",
      "Iteration 3, Episode 65/100, Average Reward: 0.12165528246926016\n",
      "Iteration 3, Episode 66/100, Average Reward: 0.12490520203803593\n",
      "Iteration 3, Episode 67/100, Average Reward: 0.1128278564573343\n",
      "Iteration 3, Episode 68/100, Average Reward: 0.11388361416603086\n",
      "Iteration 3, Episode 69/100, Average Reward: 0.11457878746978346\n",
      "Iteration 3, Episode 70/100, Average Reward: 0.13343748010369857\n",
      "Iteration 3, Episode 71/100, Average Reward: 0.1251620675661015\n",
      "Iteration 3, Episode 72/100, Average Reward: 0.13209428789241115\n",
      "Iteration 3, Episode 73/100, Average Reward: 0.1254432546490625\n",
      "Iteration 3, Episode 74/100, Average Reward: 0.11746411688114589\n",
      "Iteration 3, Episode 75/100, Average Reward: 0.12447919924019275\n",
      "Iteration 3, Episode 76/100, Average Reward: 0.12709832215085623\n",
      "Iteration 3, Episode 77/100, Average Reward: 0.11850576358055204\n",
      "Iteration 3, Episode 78/100, Average Reward: 0.11240647293099425\n",
      "Iteration 3, Episode 79/100, Average Reward: 0.12337321956975231\n",
      "Iteration 3, Episode 80/100, Average Reward: 0.12560321086600593\n",
      "Iteration 3, Episode 81/100, Average Reward: 0.14123000538737507\n",
      "Iteration 3, Episode 82/100, Average Reward: 0.08328978117163845\n",
      "Iteration 3, Episode 83/100, Average Reward: 0.07378060113464276\n",
      "Iteration 3, Episode 84/100, Average Reward: 0.06429076962475695\n",
      "Iteration 3, Episode 85/100, Average Reward: 0.0291988799916708\n",
      "Iteration 3, Episode 86/100, Average Reward: 0.05802251832257088\n",
      "Iteration 3, Episode 87/100, Average Reward: 0.08133157642606548\n",
      "Iteration 3, Episode 88/100, Average Reward: 0.08872557944126698\n",
      "Iteration 3, Episode 89/100, Average Reward: 0.10742889856449417\n",
      "Iteration 3, Episode 90/100, Average Reward: 0.12918681392807574\n",
      "Iteration 3, Episode 91/100, Average Reward: 0.12528986293887745\n",
      "Iteration 3, Episode 92/100, Average Reward: 0.13631312861499909\n",
      "Iteration 3, Episode 93/100, Average Reward: 0.11767262721194001\n",
      "Iteration 3, Episode 94/100, Average Reward: 0.12939050840153915\n",
      "Iteration 3, Episode 95/100, Average Reward: 0.13069776850968018\n",
      "Iteration 3, Episode 96/100, Average Reward: 0.15421413905209708\n",
      "Iteration 3, Episode 97/100, Average Reward: 0.1184375022354004\n",
      "Iteration 3, Episode 98/100, Average Reward: 0.12403247718355448\n",
      "Iteration 3, Episode 99/100, Average Reward: 0.117144456137854\n",
      "Iteration 3, Episode 100/100, Average Reward: 0.11977222180067086\n",
      "\n",
      "--- Training for Period 4 ---\n",
      "Loaded actor model from saved_models/last_actor_7-4.pth and critic model from saved_models/last_critic_7-4.pth\n",
      "Iteration 4, Episode 1/100, Average Reward: 0.04948722105214732\n",
      "Iteration 4, Episode 2/100, Average Reward: 0.049257586259690185\n",
      "Iteration 4, Episode 3/100, Average Reward: 0.04982664164703732\n",
      "Iteration 4, Episode 4/100, Average Reward: 0.05467683450081034\n",
      "Iteration 4, Episode 5/100, Average Reward: 0.05293578800684353\n",
      "Iteration 4, Episode 6/100, Average Reward: 0.04927026388717593\n",
      "Iteration 4, Episode 7/100, Average Reward: 0.045258169855249275\n",
      "Iteration 4, Episode 8/100, Average Reward: 0.05377795567115756\n",
      "Iteration 4, Episode 9/100, Average Reward: 0.049901706588024855\n",
      "Iteration 4, Episode 10/100, Average Reward: 0.04750344701301121\n",
      "Iteration 4, Episode 11/100, Average Reward: 0.05205919329639249\n",
      "Iteration 4, Episode 12/100, Average Reward: 0.047351215652611786\n",
      "Iteration 4, Episode 13/100, Average Reward: 0.047162194372008234\n",
      "Iteration 4, Episode 14/100, Average Reward: 0.05435415049420058\n",
      "Iteration 4, Episode 15/100, Average Reward: 0.04963246725414563\n",
      "Iteration 4, Episode 16/100, Average Reward: 0.061736002329329874\n",
      "Iteration 4, Episode 17/100, Average Reward: 0.054521400153645774\n",
      "Iteration 4, Episode 18/100, Average Reward: 0.001276181420108238\n",
      "Iteration 4, Episode 19/100, Average Reward: -0.055475199040322984\n",
      "Iteration 4, Episode 20/100, Average Reward: 0.06155052772024005\n",
      "Iteration 4, Episode 21/100, Average Reward: 0.07028342592509164\n",
      "Iteration 4, Episode 22/100, Average Reward: 0.055396456399640324\n",
      "Iteration 4, Episode 23/100, Average Reward: 0.07086594313557566\n",
      "Iteration 4, Episode 24/100, Average Reward: 0.04286981652655276\n",
      "Iteration 4, Episode 25/100, Average Reward: 0.043538400835868706\n",
      "Iteration 4, Episode 26/100, Average Reward: 0.04320273542829899\n",
      "Iteration 4, Episode 27/100, Average Reward: 0.05853855709971683\n",
      "Iteration 4, Episode 28/100, Average Reward: 0.04755421561240002\n",
      "Iteration 4, Episode 29/100, Average Reward: 0.04234074474907146\n",
      "Iteration 4, Episode 30/100, Average Reward: 0.04913131411558677\n",
      "Iteration 4, Episode 31/100, Average Reward: 0.05611973109223305\n",
      "Iteration 4, Episode 32/100, Average Reward: 0.0488544466527134\n",
      "Iteration 4, Episode 33/100, Average Reward: 0.06898845923283849\n",
      "Iteration 4, Episode 34/100, Average Reward: 0.04549768013486817\n",
      "Iteration 4, Episode 35/100, Average Reward: 0.05438441356027027\n",
      "Iteration 4, Episode 36/100, Average Reward: 0.05567978862516667\n",
      "Iteration 4, Episode 37/100, Average Reward: 0.05114492506296963\n",
      "Iteration 4, Episode 38/100, Average Reward: 0.05110624589997526\n",
      "Iteration 4, Episode 39/100, Average Reward: 0.05014799145270664\n",
      "Iteration 4, Episode 40/100, Average Reward: 0.057321135569824745\n",
      "Iteration 4, Episode 41/100, Average Reward: 0.04479839250303193\n",
      "Iteration 4, Episode 42/100, Average Reward: 0.05501290131615123\n",
      "Iteration 4, Episode 43/100, Average Reward: 0.043494123007956305\n",
      "Iteration 4, Episode 44/100, Average Reward: 0.047158844018972516\n",
      "Iteration 4, Episode 45/100, Average Reward: 0.05233828892687256\n",
      "Iteration 4, Episode 46/100, Average Reward: 0.04619098274172694\n",
      "Iteration 4, Episode 47/100, Average Reward: 0.03735336028860843\n",
      "Iteration 4, Episode 48/100, Average Reward: 0.07434965930161556\n",
      "Iteration 4, Episode 49/100, Average Reward: 0.08319026065297884\n",
      "Iteration 4, Episode 50/100, Average Reward: 0.06782198836070799\n",
      "Iteration 4, Episode 51/100, Average Reward: 0.09223780349405497\n",
      "Iteration 4, Episode 52/100, Average Reward: 0.03783814980863135\n",
      "Iteration 4, Episode 53/100, Average Reward: 0.017918133223363955\n",
      "Iteration 4, Episode 54/100, Average Reward: 0.09247619613999536\n",
      "Iteration 4, Episode 55/100, Average Reward: 0.04602896865250599\n",
      "Iteration 4, Episode 56/100, Average Reward: 0.06795827245270503\n",
      "Iteration 4, Episode 57/100, Average Reward: 0.04156548343690647\n",
      "Iteration 4, Episode 58/100, Average Reward: -0.015636989457507144\n",
      "Iteration 4, Episode 59/100, Average Reward: -0.003398976698823031\n",
      "Iteration 4, Episode 60/100, Average Reward: 0.020634725554728502\n",
      "Iteration 4, Episode 61/100, Average Reward: 0.05703680495248033\n",
      "Iteration 4, Episode 62/100, Average Reward: 0.05002143829328286\n",
      "Iteration 4, Episode 63/100, Average Reward: 0.0419961932173433\n",
      "Iteration 4, Episode 64/100, Average Reward: 0.03840979335444782\n",
      "Iteration 4, Episode 65/100, Average Reward: 0.046657844005018934\n",
      "Iteration 4, Episode 66/100, Average Reward: 0.05671378947539639\n",
      "Iteration 4, Episode 67/100, Average Reward: 0.05704406514210245\n",
      "Iteration 4, Episode 68/100, Average Reward: 0.054019189269145035\n",
      "Iteration 4, Episode 69/100, Average Reward: 0.045765272457135533\n",
      "Iteration 4, Episode 70/100, Average Reward: 0.05499013324279058\n",
      "Iteration 4, Episode 71/100, Average Reward: 0.05506859316979526\n",
      "Iteration 4, Episode 72/100, Average Reward: 0.051710847820522834\n",
      "Iteration 4, Episode 73/100, Average Reward: 0.05660957488078705\n",
      "Iteration 4, Episode 74/100, Average Reward: 0.04746765586515296\n",
      "Iteration 4, Episode 75/100, Average Reward: 0.05498022344048074\n",
      "Iteration 4, Episode 76/100, Average Reward: 0.04518252829264225\n",
      "Iteration 4, Episode 77/100, Average Reward: 0.05129755871314354\n",
      "Iteration 4, Episode 78/100, Average Reward: 0.057912802588492004\n",
      "Iteration 4, Episode 79/100, Average Reward: 0.050301960074669375\n",
      "Iteration 4, Episode 80/100, Average Reward: 0.04311411827228735\n",
      "Iteration 4, Episode 81/100, Average Reward: 0.04493126175531035\n",
      "Iteration 4, Episode 82/100, Average Reward: 0.04073861500124378\n",
      "Iteration 4, Episode 83/100, Average Reward: 0.05242316126452492\n",
      "Iteration 4, Episode 84/100, Average Reward: 0.057166949812542715\n",
      "Iteration 4, Episode 85/100, Average Reward: 0.05236164768902542\n",
      "Iteration 4, Episode 86/100, Average Reward: 0.04722470820780173\n",
      "Iteration 4, Episode 87/100, Average Reward: 0.05019248640203478\n",
      "Iteration 4, Episode 88/100, Average Reward: 0.051036329111568586\n",
      "Iteration 4, Episode 89/100, Average Reward: 0.051979393687510124\n",
      "Iteration 4, Episode 90/100, Average Reward: 0.04916462893980822\n",
      "Iteration 4, Episode 91/100, Average Reward: 0.05500976434385731\n",
      "Iteration 4, Episode 92/100, Average Reward: 0.046080632576056615\n",
      "Iteration 4, Episode 93/100, Average Reward: 0.05267287639368736\n",
      "Iteration 4, Episode 94/100, Average Reward: 0.051938499804056\n",
      "Iteration 4, Episode 95/100, Average Reward: 0.044767488134683224\n",
      "Iteration 4, Episode 96/100, Average Reward: 0.08582251910448822\n",
      "Iteration 4, Episode 97/100, Average Reward: 0.04112830789954465\n",
      "Iteration 4, Episode 98/100, Average Reward: 0.04209280577078035\n",
      "Iteration 4, Episode 99/100, Average Reward: 0.012172251809387387\n",
      "Iteration 4, Episode 100/100, Average Reward: 0.04968991880133222\n",
      "\n",
      "--- Training for Period 5 ---\n",
      "Loaded actor model from saved_models/last_actor_7-5.pth and critic model from saved_models/last_critic_7-5.pth\n",
      "Iteration 5, Episode 1/100, Average Reward: 0.08390114936429433\n",
      "Iteration 5, Episode 2/100, Average Reward: 0.07278933802762257\n",
      "Iteration 5, Episode 3/100, Average Reward: 0.07998098312214266\n",
      "Iteration 5, Episode 4/100, Average Reward: 0.0709219305437635\n",
      "Iteration 5, Episode 5/100, Average Reward: 0.07487718148691026\n",
      "Iteration 5, Episode 6/100, Average Reward: 0.07160815160364539\n",
      "Iteration 5, Episode 7/100, Average Reward: 0.07558658168490837\n",
      "Iteration 5, Episode 8/100, Average Reward: 0.07417579287272784\n",
      "Iteration 5, Episode 9/100, Average Reward: 0.07664147918571865\n",
      "Iteration 5, Episode 10/100, Average Reward: 0.0701287583704688\n",
      "Iteration 5, Episode 11/100, Average Reward: 0.08195266601676074\n",
      "Iteration 5, Episode 12/100, Average Reward: 0.07607458186336205\n",
      "Iteration 5, Episode 13/100, Average Reward: 0.0745940062872876\n",
      "Iteration 5, Episode 14/100, Average Reward: 0.07811402736608902\n",
      "Iteration 5, Episode 15/100, Average Reward: 0.0697873803297505\n",
      "Iteration 5, Episode 16/100, Average Reward: 0.07681934387828825\n",
      "Iteration 5, Episode 17/100, Average Reward: 0.07455091744064507\n",
      "Iteration 5, Episode 18/100, Average Reward: 0.10203078215462114\n",
      "Iteration 5, Episode 19/100, Average Reward: 0.05725746929544544\n",
      "Iteration 5, Episode 20/100, Average Reward: 0.06038596267985724\n",
      "Iteration 5, Episode 21/100, Average Reward: 0.08954169665765258\n",
      "Iteration 5, Episode 22/100, Average Reward: 0.07841957945242044\n",
      "Iteration 5, Episode 23/100, Average Reward: 0.07862149812127704\n",
      "Iteration 5, Episode 24/100, Average Reward: 0.07151703045246627\n",
      "Iteration 5, Episode 25/100, Average Reward: 0.06722351378295553\n",
      "Iteration 5, Episode 26/100, Average Reward: 0.07185725831308991\n",
      "Iteration 5, Episode 27/100, Average Reward: 0.07403952890360918\n",
      "Iteration 5, Episode 28/100, Average Reward: 0.06729189580485091\n",
      "Iteration 5, Episode 29/100, Average Reward: 0.08132579240407227\n",
      "Iteration 5, Episode 30/100, Average Reward: 0.08456301239072088\n",
      "Iteration 5, Episode 31/100, Average Reward: 0.07038925287041717\n",
      "Iteration 5, Episode 32/100, Average Reward: 0.07141120878435342\n",
      "Iteration 5, Episode 33/100, Average Reward: 0.07263920841771197\n",
      "Iteration 5, Episode 34/100, Average Reward: 0.09933659507628635\n",
      "Iteration 5, Episode 35/100, Average Reward: 0.0733826429205361\n",
      "Iteration 5, Episode 36/100, Average Reward: 0.07505068967228644\n",
      "Iteration 5, Episode 37/100, Average Reward: 0.07258940066968529\n",
      "Iteration 5, Episode 38/100, Average Reward: 0.07211580121686328\n",
      "Iteration 5, Episode 39/100, Average Reward: 0.07305604229140428\n",
      "Iteration 5, Episode 40/100, Average Reward: 0.07197600720886663\n",
      "Iteration 5, Episode 41/100, Average Reward: 0.08048157463587434\n",
      "Iteration 5, Episode 42/100, Average Reward: 0.07676310120444323\n",
      "Iteration 5, Episode 43/100, Average Reward: 0.08691212847672687\n",
      "Iteration 5, Episode 44/100, Average Reward: 0.06369592972575171\n",
      "Iteration 5, Episode 45/100, Average Reward: 0.06456448165303251\n",
      "Iteration 5, Episode 46/100, Average Reward: 0.06697956543039552\n",
      "Iteration 5, Episode 47/100, Average Reward: 0.07521344355010122\n",
      "Iteration 5, Episode 48/100, Average Reward: 0.08160197427076539\n",
      "Iteration 5, Episode 49/100, Average Reward: 0.07479796340713571\n",
      "Iteration 5, Episode 50/100, Average Reward: 0.06612581721444767\n",
      "Iteration 5, Episode 51/100, Average Reward: 0.07075335930783024\n",
      "Iteration 5, Episode 52/100, Average Reward: 0.07436169616721935\n",
      "Iteration 5, Episode 53/100, Average Reward: 0.07243059747120101\n",
      "Iteration 5, Episode 54/100, Average Reward: 0.06919996811189517\n",
      "Iteration 5, Episode 55/100, Average Reward: 0.07732139055113728\n",
      "Iteration 5, Episode 56/100, Average Reward: 0.07797713391335122\n",
      "Iteration 5, Episode 57/100, Average Reward: 0.08151060291835618\n",
      "Iteration 5, Episode 58/100, Average Reward: 0.07734228840246213\n",
      "Iteration 5, Episode 59/100, Average Reward: 0.06228251470293996\n",
      "Iteration 5, Episode 60/100, Average Reward: 0.06563007327081977\n",
      "Iteration 5, Episode 61/100, Average Reward: 0.0822141930625577\n",
      "Iteration 5, Episode 62/100, Average Reward: 0.07712198646799198\n",
      "Iteration 5, Episode 63/100, Average Reward: 0.061018830571298574\n",
      "Iteration 5, Episode 64/100, Average Reward: 0.0665472764851755\n",
      "Iteration 5, Episode 65/100, Average Reward: 0.0751446467483407\n",
      "Iteration 5, Episode 66/100, Average Reward: 0.07622113716289398\n",
      "Iteration 5, Episode 67/100, Average Reward: 0.10266182865371035\n",
      "Iteration 5, Episode 68/100, Average Reward: 0.0824993985651498\n",
      "Iteration 5, Episode 69/100, Average Reward: 0.06263457663683375\n",
      "Iteration 5, Episode 70/100, Average Reward: 0.061959347704333964\n",
      "Iteration 5, Episode 71/100, Average Reward: 0.06613075049453598\n",
      "Iteration 5, Episode 72/100, Average Reward: 0.06972306808672124\n",
      "Iteration 5, Episode 73/100, Average Reward: 0.08857235278453444\n",
      "Iteration 5, Episode 74/100, Average Reward: 0.06720923904589009\n",
      "Iteration 5, Episode 75/100, Average Reward: 0.0807347143399739\n",
      "Iteration 5, Episode 76/100, Average Reward: 0.06940785828886456\n",
      "Iteration 5, Episode 77/100, Average Reward: 0.06341331698361499\n",
      "Iteration 5, Episode 78/100, Average Reward: 0.08422862041602806\n",
      "Iteration 5, Episode 79/100, Average Reward: 0.07021531253184102\n",
      "Iteration 5, Episode 80/100, Average Reward: 0.06758146839668108\n",
      "Iteration 5, Episode 81/100, Average Reward: 0.07532476832136936\n",
      "Iteration 5, Episode 82/100, Average Reward: 0.0674026040113401\n",
      "Iteration 5, Episode 83/100, Average Reward: 0.08450501743627732\n",
      "Iteration 5, Episode 84/100, Average Reward: 0.07230898839992345\n",
      "Iteration 5, Episode 85/100, Average Reward: 0.07681075319328509\n",
      "Iteration 5, Episode 86/100, Average Reward: 0.07194688996450845\n",
      "Iteration 5, Episode 87/100, Average Reward: 0.09261477322504896\n",
      "Iteration 5, Episode 88/100, Average Reward: 0.1354423272217571\n",
      "Iteration 5, Episode 89/100, Average Reward: 0.1007474050032522\n",
      "Iteration 5, Episode 90/100, Average Reward: 0.11305465953730001\n",
      "Iteration 5, Episode 91/100, Average Reward: 0.09012602772688602\n",
      "Iteration 5, Episode 92/100, Average Reward: 0.06482961250253283\n",
      "Iteration 5, Episode 93/100, Average Reward: 0.07281564497477057\n",
      "Iteration 5, Episode 94/100, Average Reward: 0.07129006467824886\n",
      "Iteration 5, Episode 95/100, Average Reward: 0.07972037196244812\n",
      "Iteration 5, Episode 96/100, Average Reward: 0.08997278613523585\n",
      "Iteration 5, Episode 97/100, Average Reward: 0.07465116154172426\n",
      "Iteration 5, Episode 98/100, Average Reward: 0.07712044213363975\n",
      "Iteration 5, Episode 99/100, Average Reward: 0.07828886263590398\n",
      "Iteration 5, Episode 100/100, Average Reward: 0.06393340069804905\n",
      "\n",
      "--- Training for Period 6 ---\n",
      "Loaded actor model from saved_models/last_actor_7-6.pth and critic model from saved_models/last_critic_7-6.pth\n",
      "Iteration 6, Episode 1/100, Average Reward: 0.0501187627563047\n",
      "Iteration 6, Episode 2/100, Average Reward: 0.0746367538145526\n",
      "Iteration 6, Episode 3/100, Average Reward: 0.06387996766965093\n",
      "Iteration 6, Episode 4/100, Average Reward: 0.06466001340866626\n",
      "Iteration 6, Episode 5/100, Average Reward: 0.0791062633672189\n",
      "Iteration 6, Episode 6/100, Average Reward: 0.06958847250852115\n",
      "Iteration 6, Episode 7/100, Average Reward: 0.06392911383533686\n",
      "Iteration 6, Episode 8/100, Average Reward: 0.07378459458834455\n",
      "Iteration 6, Episode 9/100, Average Reward: 0.07009624937735894\n",
      "Iteration 6, Episode 10/100, Average Reward: 0.08231567772709558\n",
      "Iteration 6, Episode 11/100, Average Reward: 0.07276472223283542\n",
      "Iteration 6, Episode 12/100, Average Reward: 0.07508422697044362\n",
      "Iteration 6, Episode 13/100, Average Reward: 0.09077852914980826\n",
      "Iteration 6, Episode 14/100, Average Reward: 0.06505069694508646\n",
      "Iteration 6, Episode 15/100, Average Reward: 0.08299826089444905\n",
      "Iteration 6, Episode 16/100, Average Reward: 0.06054110903095841\n",
      "Iteration 6, Episode 17/100, Average Reward: 0.07387691539153099\n",
      "Iteration 6, Episode 18/100, Average Reward: 0.07936164389414381\n",
      "Iteration 6, Episode 19/100, Average Reward: 0.07710654559579228\n",
      "Iteration 6, Episode 20/100, Average Reward: 0.07481021122306297\n",
      "Iteration 6, Episode 21/100, Average Reward: 0.08036158248285644\n",
      "Iteration 6, Episode 22/100, Average Reward: 0.07068992784698254\n",
      "Iteration 6, Episode 23/100, Average Reward: 0.08203321467964636\n",
      "Iteration 6, Episode 24/100, Average Reward: 0.07544694469381498\n",
      "Iteration 6, Episode 25/100, Average Reward: 0.07259020250368983\n",
      "Iteration 6, Episode 26/100, Average Reward: 0.08382746485378738\n",
      "Iteration 6, Episode 27/100, Average Reward: 0.06780392386146304\n",
      "Iteration 6, Episode 28/100, Average Reward: 0.08180669987946412\n",
      "Iteration 6, Episode 29/100, Average Reward: 0.07585475597066682\n",
      "Iteration 6, Episode 30/100, Average Reward: 0.07025820165479627\n",
      "Iteration 6, Episode 31/100, Average Reward: 0.06984625163620879\n",
      "Iteration 6, Episode 32/100, Average Reward: 0.08278088206964444\n",
      "Iteration 6, Episode 33/100, Average Reward: 0.07120321799137572\n",
      "Iteration 6, Episode 34/100, Average Reward: 0.0728761018244555\n",
      "Iteration 6, Episode 35/100, Average Reward: 0.06979914481335614\n",
      "Iteration 6, Episode 36/100, Average Reward: 0.06709560007996529\n",
      "Iteration 6, Episode 37/100, Average Reward: 0.08189406824308161\n",
      "Iteration 6, Episode 38/100, Average Reward: 0.07342905939802927\n",
      "Iteration 6, Episode 39/100, Average Reward: 0.07615172198803559\n",
      "Iteration 6, Episode 40/100, Average Reward: 0.06426390021555446\n",
      "Iteration 6, Episode 41/100, Average Reward: 0.07608809738073997\n",
      "Iteration 6, Episode 42/100, Average Reward: 0.07245381411503493\n",
      "Iteration 6, Episode 43/100, Average Reward: 0.07149407349776518\n",
      "Iteration 6, Episode 44/100, Average Reward: 0.06874652997993\n",
      "Iteration 6, Episode 45/100, Average Reward: 0.0798026467108794\n",
      "Iteration 6, Episode 46/100, Average Reward: 0.08168573582413266\n",
      "Iteration 6, Episode 47/100, Average Reward: 0.04890855111547474\n",
      "Iteration 6, Episode 48/100, Average Reward: 0.06667178419770517\n",
      "Iteration 6, Episode 49/100, Average Reward: 0.08434372581032883\n",
      "Iteration 6, Episode 50/100, Average Reward: 0.07227409123161019\n",
      "Iteration 6, Episode 51/100, Average Reward: 0.0637365376962849\n",
      "Iteration 6, Episode 52/100, Average Reward: 0.055034718089767856\n",
      "Iteration 6, Episode 53/100, Average Reward: 0.09609671643862992\n",
      "Iteration 6, Episode 54/100, Average Reward: 0.07079008289714221\n",
      "Iteration 6, Episode 55/100, Average Reward: 0.07878077248819657\n",
      "Iteration 6, Episode 56/100, Average Reward: 0.0704128877432974\n",
      "Iteration 6, Episode 57/100, Average Reward: 0.06922524934073412\n",
      "Iteration 6, Episode 58/100, Average Reward: 0.06518111408326051\n",
      "Iteration 6, Episode 59/100, Average Reward: 0.07251027657223857\n",
      "Iteration 6, Episode 60/100, Average Reward: 0.06825979712216798\n",
      "Iteration 6, Episode 61/100, Average Reward: 0.05756817526123209\n",
      "Iteration 6, Episode 62/100, Average Reward: 0.0429794606304519\n",
      "Iteration 6, Episode 63/100, Average Reward: 0.07599697233727201\n",
      "Iteration 6, Episode 64/100, Average Reward: 0.07130863170396204\n",
      "Iteration 6, Episode 65/100, Average Reward: 0.08548867782972841\n",
      "Iteration 6, Episode 66/100, Average Reward: 0.07740362542934863\n",
      "Iteration 6, Episode 67/100, Average Reward: 0.06774682976501267\n",
      "Iteration 6, Episode 68/100, Average Reward: 0.10735453376938407\n",
      "Iteration 6, Episode 69/100, Average Reward: 0.06863907311298835\n",
      "Iteration 6, Episode 70/100, Average Reward: 0.07806128147825624\n",
      "Iteration 6, Episode 71/100, Average Reward: 0.06298636922667991\n",
      "Iteration 6, Episode 72/100, Average Reward: 0.09331735878985474\n",
      "Iteration 6, Episode 73/100, Average Reward: 0.056854002602411406\n",
      "Iteration 6, Episode 74/100, Average Reward: 0.08401657079155014\n",
      "Iteration 6, Episode 75/100, Average Reward: 0.06282791586641136\n",
      "Iteration 6, Episode 76/100, Average Reward: 0.07713144765772999\n",
      "Iteration 6, Episode 77/100, Average Reward: 0.07536977733221167\n",
      "Iteration 6, Episode 78/100, Average Reward: 0.07346408658990802\n",
      "Iteration 6, Episode 79/100, Average Reward: 0.07598767346851218\n",
      "Iteration 6, Episode 80/100, Average Reward: 0.07541596217843521\n",
      "Iteration 6, Episode 81/100, Average Reward: 0.0802745612302864\n",
      "Iteration 6, Episode 82/100, Average Reward: 0.0780287454179515\n",
      "Iteration 6, Episode 83/100, Average Reward: 0.06663530702020005\n",
      "Iteration 6, Episode 84/100, Average Reward: 0.06889228298484927\n",
      "Iteration 6, Episode 85/100, Average Reward: 0.07251499132107701\n",
      "Iteration 6, Episode 86/100, Average Reward: 0.08677426374170834\n",
      "Iteration 6, Episode 87/100, Average Reward: 0.07188680856234035\n",
      "Iteration 6, Episode 88/100, Average Reward: 0.07157710936981006\n",
      "Iteration 6, Episode 89/100, Average Reward: 0.08067953867661426\n",
      "Iteration 6, Episode 90/100, Average Reward: 0.08457195799906096\n",
      "Iteration 6, Episode 91/100, Average Reward: 0.06929722166256976\n",
      "Iteration 6, Episode 92/100, Average Reward: 0.071333578457365\n",
      "Iteration 6, Episode 93/100, Average Reward: 0.08039081021858084\n",
      "Iteration 6, Episode 94/100, Average Reward: 0.07410899566110814\n",
      "Iteration 6, Episode 95/100, Average Reward: 0.06613517467970137\n",
      "Iteration 6, Episode 96/100, Average Reward: 0.09165784404120572\n",
      "Iteration 6, Episode 97/100, Average Reward: 0.06939762528108283\n",
      "Iteration 6, Episode 98/100, Average Reward: 0.05993951408823494\n",
      "Iteration 6, Episode 99/100, Average Reward: 0.07340126514796326\n",
      "Iteration 6, Episode 100/100, Average Reward: 0.06599016332006376\n",
      "\n",
      "--- Training for Period 7 ---\n",
      "Loaded actor model from saved_models/last_actor_7-7.pth and critic model from saved_models/last_critic_7-7.pth\n",
      "Iteration 7, Episode 1/100, Average Reward: 0.03030012961173787\n",
      "Iteration 7, Episode 2/100, Average Reward: 0.09223322169515173\n",
      "Iteration 7, Episode 3/100, Average Reward: 0.08973236840733491\n",
      "Iteration 7, Episode 4/100, Average Reward: 0.080995019569124\n",
      "Iteration 7, Episode 5/100, Average Reward: 0.08819983145562449\n",
      "Iteration 7, Episode 6/100, Average Reward: 0.09378121350288109\n",
      "Iteration 7, Episode 7/100, Average Reward: 0.0869134088734056\n",
      "Iteration 7, Episode 8/100, Average Reward: 0.10071138553153672\n",
      "Iteration 7, Episode 9/100, Average Reward: 0.09709756647610666\n",
      "Iteration 7, Episode 10/100, Average Reward: 0.09424074853661028\n",
      "Iteration 7, Episode 11/100, Average Reward: 0.10207212782285602\n",
      "Iteration 7, Episode 12/100, Average Reward: 0.07315966987947216\n",
      "Iteration 7, Episode 13/100, Average Reward: 0.09332664777783785\n",
      "Iteration 7, Episode 14/100, Average Reward: 0.09389051437677952\n",
      "Iteration 7, Episode 15/100, Average Reward: 0.09229160218676535\n",
      "Iteration 7, Episode 16/100, Average Reward: 0.09196721890791061\n",
      "Iteration 7, Episode 17/100, Average Reward: 0.11327725429030636\n",
      "Iteration 7, Episode 18/100, Average Reward: 0.08834492880785187\n",
      "Iteration 7, Episode 19/100, Average Reward: 0.09432990888796809\n",
      "Iteration 7, Episode 20/100, Average Reward: 0.09624814383607407\n",
      "Iteration 7, Episode 21/100, Average Reward: 0.10092360860499018\n",
      "Iteration 7, Episode 22/100, Average Reward: 0.09695682631465606\n",
      "Iteration 7, Episode 23/100, Average Reward: 0.08453559664513702\n",
      "Iteration 7, Episode 24/100, Average Reward: 0.09376793083284303\n",
      "Iteration 7, Episode 25/100, Average Reward: 0.0875278281201393\n",
      "Iteration 7, Episode 26/100, Average Reward: 0.09004574438768376\n",
      "Iteration 7, Episode 27/100, Average Reward: 0.08877444853990075\n",
      "Iteration 7, Episode 28/100, Average Reward: 0.09749784561766167\n",
      "Iteration 7, Episode 29/100, Average Reward: 0.09265465846106945\n",
      "Iteration 7, Episode 30/100, Average Reward: 0.09171431482462834\n",
      "Iteration 7, Episode 31/100, Average Reward: 0.09605082301055291\n",
      "Iteration 7, Episode 32/100, Average Reward: 0.09434800734670852\n",
      "Iteration 7, Episode 33/100, Average Reward: 0.08437526580180046\n",
      "Iteration 7, Episode 34/100, Average Reward: 0.09040147613816386\n",
      "Iteration 7, Episode 35/100, Average Reward: 0.09277135464511305\n",
      "Iteration 7, Episode 36/100, Average Reward: 0.09289431249174604\n",
      "Iteration 7, Episode 37/100, Average Reward: 0.09112972072742273\n",
      "Iteration 7, Episode 38/100, Average Reward: 0.08800870283538391\n",
      "Iteration 7, Episode 39/100, Average Reward: 0.08161342213419172\n",
      "Iteration 7, Episode 40/100, Average Reward: 0.09128793611179348\n",
      "Iteration 7, Episode 41/100, Average Reward: 0.1027696659308649\n",
      "Iteration 7, Episode 42/100, Average Reward: 0.08636071973916233\n",
      "Iteration 7, Episode 43/100, Average Reward: 0.0994469114771346\n",
      "Iteration 7, Episode 44/100, Average Reward: 0.08787977034054874\n",
      "Iteration 7, Episode 45/100, Average Reward: 0.11720375267827554\n",
      "Iteration 7, Episode 46/100, Average Reward: 0.09072718788926394\n",
      "Iteration 7, Episode 47/100, Average Reward: 0.09544524676351086\n",
      "Iteration 7, Episode 48/100, Average Reward: 0.08988239711258511\n",
      "Iteration 7, Episode 49/100, Average Reward: 0.085406100844111\n",
      "Iteration 7, Episode 50/100, Average Reward: 0.09355348744868654\n",
      "Iteration 7, Episode 51/100, Average Reward: 0.08780566629813535\n",
      "Iteration 7, Episode 52/100, Average Reward: 0.08107381926379971\n",
      "Iteration 7, Episode 53/100, Average Reward: 0.08988361796689297\n",
      "Iteration 7, Episode 54/100, Average Reward: 0.0983363063280226\n",
      "Iteration 7, Episode 55/100, Average Reward: 0.09394812933810362\n",
      "Iteration 7, Episode 56/100, Average Reward: 0.08674144875753982\n",
      "Iteration 7, Episode 57/100, Average Reward: 0.09697770486193313\n",
      "Iteration 7, Episode 58/100, Average Reward: 0.09649683158587262\n",
      "Iteration 7, Episode 59/100, Average Reward: 0.09471103305422628\n",
      "Iteration 7, Episode 60/100, Average Reward: 0.09221995612677135\n",
      "Iteration 7, Episode 61/100, Average Reward: 0.03596402358591137\n",
      "Iteration 7, Episode 62/100, Average Reward: 0.08092424331783732\n",
      "Iteration 7, Episode 63/100, Average Reward: 0.08444784954303226\n",
      "Iteration 7, Episode 64/100, Average Reward: 0.09228495181379529\n",
      "Iteration 7, Episode 65/100, Average Reward: 0.09647098968273354\n",
      "Iteration 7, Episode 66/100, Average Reward: 0.09064423906297056\n",
      "Iteration 7, Episode 67/100, Average Reward: 0.08388972173837178\n",
      "Iteration 7, Episode 68/100, Average Reward: 0.07817749475378939\n",
      "Iteration 7, Episode 69/100, Average Reward: 0.1012073149515082\n",
      "Iteration 7, Episode 70/100, Average Reward: 0.10383832183438246\n",
      "Iteration 7, Episode 71/100, Average Reward: 0.09941256956156516\n",
      "Iteration 7, Episode 72/100, Average Reward: 0.09554192328391635\n",
      "Iteration 7, Episode 73/100, Average Reward: 0.09274881399745018\n",
      "Iteration 7, Episode 74/100, Average Reward: 0.09167159123189156\n",
      "Iteration 7, Episode 75/100, Average Reward: 0.08751420356791723\n",
      "Iteration 7, Episode 76/100, Average Reward: 0.0943986570330817\n",
      "Iteration 7, Episode 77/100, Average Reward: 0.09680591578579195\n",
      "Iteration 7, Episode 78/100, Average Reward: 0.0883097327245402\n",
      "Iteration 7, Episode 79/100, Average Reward: 0.09379922242727216\n",
      "Iteration 7, Episode 80/100, Average Reward: 0.09132320394885565\n",
      "Iteration 7, Episode 81/100, Average Reward: 0.0906178625939838\n",
      "Iteration 7, Episode 82/100, Average Reward: 0.08946800289099008\n",
      "Iteration 7, Episode 83/100, Average Reward: 0.08921891269792985\n",
      "Iteration 7, Episode 84/100, Average Reward: 0.0870890502188909\n",
      "Iteration 7, Episode 85/100, Average Reward: 0.09894760025748052\n",
      "Iteration 7, Episode 86/100, Average Reward: 0.10345232782386814\n",
      "Iteration 7, Episode 87/100, Average Reward: 0.09016811380439979\n",
      "Iteration 7, Episode 88/100, Average Reward: 0.10124796783763337\n",
      "Iteration 7, Episode 89/100, Average Reward: 0.0864258912282146\n",
      "Iteration 7, Episode 90/100, Average Reward: 0.09303949799379205\n",
      "Iteration 7, Episode 91/100, Average Reward: 0.10491622327814687\n",
      "Iteration 7, Episode 92/100, Average Reward: 0.09316018981561765\n",
      "Iteration 7, Episode 93/100, Average Reward: 0.09441761638938752\n",
      "Iteration 7, Episode 94/100, Average Reward: 0.08543896264643647\n",
      "Iteration 7, Episode 95/100, Average Reward: 0.09490192341453127\n",
      "Iteration 7, Episode 96/100, Average Reward: 0.08468185016906346\n",
      "Iteration 7, Episode 97/100, Average Reward: 0.08942415217766898\n",
      "Iteration 7, Episode 98/100, Average Reward: 0.09897944821799157\n",
      "Iteration 7, Episode 99/100, Average Reward: 0.09519166197232354\n",
      "Iteration 7, Episode 100/100, Average Reward: 0.038080382108186595\n",
      "\n",
      "--- Training for Period 8 ---\n",
      "Loaded actor model from saved_models/last_actor_7-8.pth and critic model from saved_models/last_critic_7-8.pth\n",
      "Iteration 8, Episode 1/100, Average Reward: 0.04177213260015932\n",
      "Iteration 8, Episode 2/100, Average Reward: 0.04526346863482319\n",
      "Iteration 8, Episode 3/100, Average Reward: 0.04799595256250432\n",
      "Iteration 8, Episode 4/100, Average Reward: 0.04909002124049669\n",
      "Iteration 8, Episode 5/100, Average Reward: 0.0412672553776735\n",
      "Iteration 8, Episode 6/100, Average Reward: 0.03491198000195831\n",
      "Iteration 8, Episode 7/100, Average Reward: 0.05046235736472567\n",
      "Iteration 8, Episode 8/100, Average Reward: 0.041723562774364086\n",
      "Iteration 8, Episode 9/100, Average Reward: 0.053132744150850154\n",
      "Iteration 8, Episode 10/100, Average Reward: 0.04333712535558422\n",
      "Iteration 8, Episode 11/100, Average Reward: 0.051387084198679156\n",
      "Iteration 8, Episode 12/100, Average Reward: 0.047660356093203425\n",
      "Iteration 8, Episode 13/100, Average Reward: 0.04069461352663235\n",
      "Iteration 8, Episode 14/100, Average Reward: 0.05744648440827671\n",
      "Iteration 8, Episode 15/100, Average Reward: 0.051171741994743475\n",
      "Iteration 8, Episode 16/100, Average Reward: 0.0393781919390191\n",
      "Iteration 8, Episode 17/100, Average Reward: 0.048772992962415926\n",
      "Iteration 8, Episode 18/100, Average Reward: 0.05150455666172954\n",
      "Iteration 8, Episode 19/100, Average Reward: 0.04860496196972248\n",
      "Iteration 8, Episode 20/100, Average Reward: 0.0517322366629548\n",
      "Iteration 8, Episode 21/100, Average Reward: 0.050678797018709594\n",
      "Iteration 8, Episode 22/100, Average Reward: 0.053356732075939606\n",
      "Iteration 8, Episode 23/100, Average Reward: 0.04603675591566491\n",
      "Iteration 8, Episode 24/100, Average Reward: 0.04588378685853259\n",
      "Iteration 8, Episode 25/100, Average Reward: 0.04711949647475609\n",
      "Iteration 8, Episode 26/100, Average Reward: 0.045531341032199926\n",
      "Iteration 8, Episode 27/100, Average Reward: 0.04722188235724302\n",
      "Iteration 8, Episode 28/100, Average Reward: 0.053797973896725235\n",
      "Iteration 8, Episode 29/100, Average Reward: 0.04093533909235741\n",
      "Iteration 8, Episode 30/100, Average Reward: 0.05626083632434588\n",
      "Iteration 8, Episode 31/100, Average Reward: 0.04758095714710223\n",
      "Iteration 8, Episode 32/100, Average Reward: 0.05045060839624749\n",
      "Iteration 8, Episode 33/100, Average Reward: 0.05353805832065363\n",
      "Iteration 8, Episode 34/100, Average Reward: 0.034483587855413625\n",
      "Iteration 8, Episode 35/100, Average Reward: 0.045771523507359135\n",
      "Iteration 8, Episode 36/100, Average Reward: 0.04871801274409159\n",
      "Iteration 8, Episode 37/100, Average Reward: 0.05263005157497831\n",
      "Iteration 8, Episode 38/100, Average Reward: 0.04846567758825751\n",
      "Iteration 8, Episode 39/100, Average Reward: 0.05028325146324942\n",
      "Iteration 8, Episode 40/100, Average Reward: 0.05150021248937743\n",
      "Iteration 8, Episode 41/100, Average Reward: 0.055312369332801314\n",
      "Iteration 8, Episode 42/100, Average Reward: 0.04793639969011649\n",
      "Iteration 8, Episode 43/100, Average Reward: 0.047441582275663766\n",
      "Iteration 8, Episode 44/100, Average Reward: 0.05343347074897068\n",
      "Iteration 8, Episode 45/100, Average Reward: 0.046384123117915205\n",
      "Iteration 8, Episode 46/100, Average Reward: 0.050840912480979994\n",
      "Iteration 8, Episode 47/100, Average Reward: 0.045193129906695356\n",
      "Iteration 8, Episode 48/100, Average Reward: 0.0479958749808317\n",
      "Iteration 8, Episode 49/100, Average Reward: 0.0477614088037401\n",
      "Iteration 8, Episode 50/100, Average Reward: 0.04685143504919755\n",
      "Iteration 8, Episode 51/100, Average Reward: 0.03628768008609601\n",
      "Iteration 8, Episode 52/100, Average Reward: 0.050728094850472555\n",
      "Iteration 8, Episode 53/100, Average Reward: 0.05296816276596636\n",
      "Iteration 8, Episode 54/100, Average Reward: 0.04335619990145636\n",
      "Iteration 8, Episode 55/100, Average Reward: 0.05156222406237107\n",
      "Iteration 8, Episode 56/100, Average Reward: 0.05354662899385049\n",
      "Iteration 8, Episode 57/100, Average Reward: 0.056475311791003444\n",
      "Iteration 8, Episode 58/100, Average Reward: 0.06265057700334466\n",
      "Iteration 8, Episode 59/100, Average Reward: 0.04999445614113156\n",
      "Iteration 8, Episode 60/100, Average Reward: 0.041064422297561694\n",
      "Iteration 8, Episode 61/100, Average Reward: 0.04468937988183533\n",
      "Iteration 8, Episode 62/100, Average Reward: 0.03200413253408366\n",
      "Iteration 8, Episode 63/100, Average Reward: 0.04640608518029662\n",
      "Iteration 8, Episode 64/100, Average Reward: 0.0347184709501087\n",
      "Iteration 8, Episode 65/100, Average Reward: 0.05470918048686024\n",
      "Iteration 8, Episode 66/100, Average Reward: 0.03826848958041177\n",
      "Iteration 8, Episode 67/100, Average Reward: 0.0504827322570274\n",
      "Iteration 8, Episode 68/100, Average Reward: 0.04101658570981193\n",
      "Iteration 8, Episode 69/100, Average Reward: 0.041513756769638796\n",
      "Iteration 8, Episode 70/100, Average Reward: 0.043992581897566024\n",
      "Iteration 8, Episode 71/100, Average Reward: 0.04647002713615278\n",
      "Iteration 8, Episode 72/100, Average Reward: 0.036848355617308176\n",
      "Iteration 8, Episode 73/100, Average Reward: 0.04932689874979015\n",
      "Iteration 8, Episode 74/100, Average Reward: 0.053757628471209934\n",
      "Iteration 8, Episode 75/100, Average Reward: 0.04598917136398006\n",
      "Iteration 8, Episode 76/100, Average Reward: 0.03066490648625752\n",
      "Iteration 8, Episode 77/100, Average Reward: 0.036871566898863915\n",
      "Iteration 8, Episode 78/100, Average Reward: 0.05199993899449727\n",
      "Iteration 8, Episode 79/100, Average Reward: 0.04399983544761926\n",
      "Iteration 8, Episode 80/100, Average Reward: 0.04173005734812411\n",
      "Iteration 8, Episode 81/100, Average Reward: 0.04483985098472879\n",
      "Iteration 8, Episode 82/100, Average Reward: 0.047467592939727944\n",
      "Iteration 8, Episode 83/100, Average Reward: 0.053343468740379554\n",
      "Iteration 8, Episode 84/100, Average Reward: 0.05163216842661381\n",
      "Iteration 8, Episode 85/100, Average Reward: 0.0480131616721214\n",
      "Iteration 8, Episode 86/100, Average Reward: 0.03441737569677056\n",
      "Iteration 8, Episode 87/100, Average Reward: 0.05218731701428549\n",
      "Iteration 8, Episode 88/100, Average Reward: 0.03471356161209804\n",
      "Iteration 8, Episode 89/100, Average Reward: 0.04983963141044917\n",
      "Iteration 8, Episode 90/100, Average Reward: 0.04985899081643218\n",
      "Iteration 8, Episode 91/100, Average Reward: 0.045839461442195986\n",
      "Iteration 8, Episode 92/100, Average Reward: 0.049117049640994156\n",
      "Iteration 8, Episode 93/100, Average Reward: 0.03755929173810402\n",
      "Iteration 8, Episode 94/100, Average Reward: 0.05542523734931316\n",
      "Iteration 8, Episode 95/100, Average Reward: 0.03349074581364797\n",
      "Iteration 8, Episode 96/100, Average Reward: 0.04586021160129921\n",
      "Iteration 8, Episode 97/100, Average Reward: 0.048230868399192134\n",
      "Iteration 8, Episode 98/100, Average Reward: 0.043507465461030986\n",
      "Iteration 8, Episode 99/100, Average Reward: 0.046601485273718955\n",
      "Iteration 8, Episode 100/100, Average Reward: 0.04229860365712204\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store rewards for each iteration\n",
    "all_rewards = {}\n",
    "\n",
    "# Loop through each period in `filtered_returns_list`\n",
    "for i in range(len(filtered_returns_list)):\n",
    "    print(f\"\\n--- Training for Period {i + 1} ---\")\n",
    "\n",
    "    # Set up the environment for the current period\n",
    "    env = PortfolioEnv(filtered_returns_list[i])\n",
    "\n",
    "    # Instantiate the agent\n",
    "    agent = DDPGAgent(num_assets=NUM_ASSETS)\n",
    "\n",
    "    # Load the models from the previous iteration\n",
    "    actor_path = f\"saved_models/last_actor_7-{i+1}.pth\"\n",
    "    critic_path = f\"saved_models/last_critic_7-{i+1}.pth\"\n",
    "    if os.path.exists(actor_path) and os.path.exists(critic_path):\n",
    "        agent.actor.load_state_dict(torch.load(actor_path))\n",
    "        agent.critic.load_state_dict(torch.load(critic_path))\n",
    "        print(f\"Loaded actor model from {actor_path} and critic model from {critic_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Missing required models: {actor_path} or {critic_path}\")\n",
    "\n",
    "    # Create an array to store rewards for the current iteration\n",
    "    rewards = []\n",
    "\n",
    "    # Training parameters\n",
    "    MAX_ITERATIONS = EPISODES  # Maximum number of episodes\n",
    "\n",
    "    # Best model tracking\n",
    "    best_avg_reward = -float('inf')  # Track the best average reward\n",
    "    best_actor_state = None          # To store the best actor state\n",
    "    best_critic_state = None         # To store the best critic state\n",
    "\n",
    "    # Training loop\n",
    "    for episode in range(MAX_ITERATIONS):\n",
    "        state = env.reset()\n",
    "        agent.reset_noise()  # Reset the OU noise process for the new episode\n",
    "\n",
    "        episode_reward = 0\n",
    "        count = 0\n",
    "\n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Store transition in memory and update agent\n",
    "            agent.memory.append((state, action, reward, next_state, done))\n",
    "            agent.update()\n",
    "\n",
    "            # Update state and episode reward\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            count += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Calculate average reward for this episode\n",
    "        ann_episode_reward = episode_reward / count * 252\n",
    "        rewards.append(ann_episode_reward)\n",
    "        print(f\"Iteration {i + 1}, Episode {episode + 1}/{MAX_ITERATIONS}, Average Reward: {ann_episode_reward}\")\n",
    "\n",
    "        # Save the best-performing model if the reward is improved\n",
    "        if ann_episode_reward > best_avg_reward:\n",
    "            best_avg_reward = ann_episode_reward\n",
    "            best_actor_state = agent.actor.state_dict()\n",
    "            best_critic_state = agent.critic.state_dict()\n",
    "\n",
    "    # Save the final best model for this period\n",
    "    torch.save(best_actor_state, f'saved_models/best_actor_7-{i+2}.pth')\n",
    "    torch.save(best_critic_state, f'saved_models/best_critic_7-{i+2}.pth')\n",
    "\n",
    "    # Save the last model for the next iteration\n",
    "    torch.save(agent.actor.state_dict(), f\"saved_models/last_actor_7-{i+2}.pth\")\n",
    "    torch.save(agent.critic.state_dict(), f\"saved_models/last_critic_7-{i+2}.pth\")\n",
    "\n",
    "    # Store rewards for this iteration\n",
    "    all_rewards[i + 1] = rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
